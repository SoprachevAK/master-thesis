#import "../utils/utils.typ": fig, subfig, todo, listing

= Обзор предметной области
== Актуальность

Обусловлена популярностью пилотной версии PolyMap. В первую неделю учебного семестра, приложением пользовалось более 6000 студентов, что составляет около 80% от всей возможной аудитории.

== Цели и задачи (В введении)

Сервис PolyMap является продолжением бакалаврской работы, в которой была реализована пилотная версия приложения, она обладала следующими ограничениями:
- Поддерживалась только iOS платформа
- Поддерживалась только одна карта Политеха, которая была жёстко закодирована в приложении
- Распространялось в виде приложения, которое требовалось устанавливать на устройство

=== Цели
Цели магистерской работы вытекают из ограничений пилотной версии приложения:
+ Разработать кроссплатформенное решение, которое будет доступно прямо в браузере, и будет адаптировано под управление как с помощью мыши на компьютере, так и с помощью сенсорного экрана на мобильных устройствах.
+ Реализовать возможность динамического просмотра разных карт, которые будут загружаться из удалённого сервера по запросу пользователя.
+ Серверная часть приложения должна справляться с вариативными нагрузками с высокими пиками. Должно быть быстрое время ответа в разных регионах мира.

=== Задачи
Для достижения поставленных целий необходимо решить следующие задачи:
+ Спроектировать и реализовать гибкую клиент-серверную архитектуру приложения
+ Разработать веб-приложение, которое будет отображать интерактивные карты в формате Extended-IMDF (формат карт, используемый в приложении PolyMap)
  + Реализовать мобильную и компьютерную версии приложений.
  + Интерфейс должен быть адаптирован под разные устройства.
  + Управление картой на мобильных устройствах должно поддерживать жесты несколькими пальцами (для вращения и приближения карты).
+ Реализовать серверную часть, которая будет хранить карты и предоставлять их пользователю по запросу, а так же обрабатывать сопутствующие запросы веб-приложения (функция поделиться, генерация QR коров, сокращение ссылок).
+ Добавить в конструктор карт возможность загрузки карты на сервер.






= Архитектура. Serverless подход

В этой главе будет рассмотрена верхнеуровневая архитектура сервиса PolyMap, будет подобран оптимальный подход к разработке серверной части, а так же будет рассмотрен выбор оптимального облачного провайдера для реализации Serverless архитектуры.

== Анализ требований и проектирование
Архитектура сервиса в первую очередь определяется требованиями и задачами, которые перед ней ставятся. Одним из подходов к постановке технического задания является использования пользовательских сценариев. Они позволяют описать требования к системе с точки зрения пользователя, что позволяет лучше понять, как система будет использоваться в реальной жизни.

=== Ролевая модель. Сценарии использования

В сервисе PolyMap выделяются три основные роли:
- *Заказчик* -- это человек, который заказывает разработку карты.
- *Пользователь* -- это человек, который использует уже созданную карту.
- *Художник* -- это человек, который по поручению заказчика оцифровывает карту в конструкторе сервиса.

При этом заказчик и художник могут быть одним и тем же человеком, так как в большинстве случаев, заказчик сам будет оцифровывать карту. Однако, в некоторых случаях, заказчик может поручить эту задачу художнику как своему представителю, так и заказать обрисовку карты у PolyMap.

#include "../assets/user-stories.typ"

Таким образом, верхнеуровневую архитектуру сервиса можно представить в виде трёх основных компонентов:
+ Клиентская часть -- приложение отображающее интерактивную карту.
+ Конструктор карт -- доступен для заказчиков карт и художников, позволяет создавать и редактировать карты, а так же загружать их на сервер.
+ Серверная часть -- отвечает за хранение карт и сопутствующих данных, а так же за обработку запросов от клиентской части.

Необходимо предусмотреть возможность расширения клиентской части до нативных мобильных приложения на iOS и Android, а так же возможность в будущем, предоставлять доступ к конструктору карт неограниченному числу лиц.

Так же важным требованием является размещение серверной части в России, что позволит снизить инфраструктурные риски, связанных с политической ситуацией в мире.

#fig("Общая схема архитектуры приложения", "../assets/client-server-db.pdf", width: 90%)

== Клиентская часть
Для соответствия требованиям кроссплатформенности, и скорости загрузки, было решено использовать веб-технологии для разработки клиентской части. Это позволит использовать единое веб-приложение как на мобильных устройствах, так и на компьютерах. Кроме того, это позволит избежать необходимости установки приложения на устройство, что значительно упростит процесс использования сервиса и даст пользователям более комфортный опыт (открыть сайт с картой быстрее и безопаснее для пользователя, чем скачивать и устанавливать приложения).

Клиентское веб-приложение всё ещё требует адаптации под разные устройства и способы пользовательского ввода. Для этого необходимо использовать адаптивный интерфейс, который будет подстраиваться под размер экрана устройства, а так же поддерживать способы управления картой мышкой и сенсорным экраном.

Клиентское приложение должно поддерживать следующие функции:
- Отображение карты в формате Extended-IMDF
  - Просмотр карты
  - Приближение и вращение карты
  - Просмотр планировок этажей
  - Переключения между этажами текущего здания
- Отображение аннотаций на карте и этажах
- Поиск по аннотациями на карте
- Просмотр информации об аннотации
  - Текстовое описание аннотации
  - Расположение аннотации на карте
- Построением маршрута по карте
  - Маршрут между двумя аннотациями
  - Маршрут от стандартной точки начала до аннотации (точка начала своя для каждой карты, их может быть несколько)
  - Маршрут должен бесшовно переходить от прилежащей территории к зданию и между этажами
- Должна быть возможность поделиться маршрутом с другими пользователями
  - С помощью постоянной URL ссылки
  - С помощью QR-кода

Подробная реализация будет рассмотрена в @web-map-implementation.

== Конструктор карт
Конструктор карт является отдельным приложением, которое доступно для художников и заказчиков карт после авторизации. В нём можно создавать и редактировать карты, выгрузка их на сервер осуществляется из специального меню конструктора, которое будет доступно только для авторизованных пользователей. При планировании архитектуры сервиса, необходимо предусмотреть будущие развитие конструктора до публичной веб-версии, к которой будет иметь доступ любой желающий. Общее видение проекта подразумевает бизнес-модель аналогичную конструкторам сайтов, таким как Tilda или Wix. То есть, любой желающий сможет спроектировать карту, загрузить её на сервер и просмотреть, однако, для того, чтоб сделать её общедоступной, необходимо будет оплатить подписку.

Реализация веб-версии конструктора выходит за рамки текущий работы, однако планируется в дальнейших обновлениях сервиса, что накладывать ряд дополнительных требований на архитектуру:
- Весь процесс взаимодействия и обработки новых карт должен быть автоматизирован.
- Необходимо предусмотреть возможность добавления сервера синхронизации для совместной работы над одной картой.
- Необходимо заложить возможность масштабирования серверной части, отвечающей за обработку новых и изменения старых карт.

== Серверная часть
Сервис будет сталкиваться с неравномерной нагрузкой с высокими пиками, которые будут возникать в связи со следующими причинами:
- Повышенный спрос на карту университетов в начале учебного семестра
- Повышенный спрос на карту университетов в начале каждого дня. По утрам, в 10:00 и в 12:00, перед началом пар, студенты будут открывать карту, чтобы найти нужную аудиторию. Что подтверждается статистикой использования пилотной версии.
#todo[Добавить график использования пилотной версии PolyMap]
- При использовании карты на временных конференциях и выставках, большинство пользователей будут открывать карту одновременно, что создаст пиковую нагрузку на сервер.

Для решения этих проблем необходимо предусмотреть горизонтальное масштабирование серверной части, для этого *был выбран микросервисных подход* к разработке.

== Serverless подход
Для оркестрации микросервисов наиболее распространено использовать Kubernetes, однако последние несколько лет, растёт популярность альтернативного подхода -- Serverless. Он позволяет запускать и масштабировать микросервисы без необходимости взаимодействовать с виртуальными или выделенными серверами. Вместо этого, Serverless предлагает арендовать у облачных провайдеров только вычислительные ресурсы, которые фактически были использованы.

Основными недостатками Kubernetes выделают сложность настройки качественной инфраструктуры, высокий риск ошибок при ручной настройке, сложность управления версиями, а так же высокую стоимость кластера, особенно на начальных этапах разработки и тестовых средах.

=== Преимущества и недостатки
Как и любой другой подход, Serverless имеет свои плюсы и минусы. Плюсы Serverless вытекают из минусов Kubernetes:
+ Простота настройки и управления -- при использовании Serverless, разработчик не взаимодействует с виртуальными серверами, а только с облачными ресурсами, которые предоставляют достаточно простой и однозначный интерфейс управления.
+ Низкая вероятность ошибок -- так как разработчик не взаимодействует с виртуальными серверами, то вероятность ошибок при настройке инфраструктуры значительно снижается. Кроме того, облачные провайдеры предоставляют таки нестойки, которые не позволят допустить существенную ошибку.
+ Низкая стоимость -- Serverless распространяется по модели pay-as-you-go, то есть необходимо оплачивать только те ресурсы, которые были фактически использованы, что хорошо видно на @serverless-vs-dedicated. Это позволяет значительно снизить затраты на инфраструктуру.

#fig(
  "Сравнение затрат на инфраструктуру",
  "../assets/serverless-vs-dedicated.pdf",
  width: 90%,
)<serverless-vs-dedicated>

За счёт того, что микросервисы в Serverless подходе запускаются только во время запроса пользователя и отключаются после завершения обработки, они потребляют ресурсы облака только в момент фактической работы, в отличии от Kubernetes, в котором вы арендуете виртуальные сервера вне зависимости от их использования. При переменно нагрузки, Kubernetes будет простаивать значительное время, что приведёт к нерациональным затратам, которых позволяет избежать Serverless.

Однако Serverless имеет и свои недостатки:
+ Требования читой архитектуры -- Serverless подход требует от разработчиков строгого соблюдения принципов чистой архитектуры:
  - микросервисы быть независимыми друг от друга
  - связи должны быть сделаны через очереди сообщений
  - микросервисы должны быть State Less -- то есть не должно храниться внутреннее состояние, оно будет потеряно при перезапуске микросервиса
+ Привязанность к облачному провайдеру -- Serverless часть есть у большинства крупных облачных провайдеров, однако конечная реализация может отличаться, что сильно усложняет миграцию между облачными провайдерами.
+ Ограниченность возможностей -- не любую микросервисную архитектуру удастся реализовать с помощью Serverless. Например, большинство баз данных не поддерживают Serverless. Однако облака предоставляют доступ к Managed версиям баз данных, к которым можно обращаться из Serverless микросервисов. Кроме того, не все технологии смогут корректно работать в условиях короткого жизненного цикла.
+ Должно быть обеспечено быстрое время холодного старта -- так как микросервисы запускаются только во время запроса, то время их запуска должно быть минимальным. Это накладывает ограничения на используемые технологии, подробнее о выборе подходящего технологического стека рассмотрено в @serverless-technology-stack.

=== Cloud Native
В современном мире, для разработки и развёртывания приложений, всё чаще используется Cloud Native подход. Он подразумевает разработку приложений, которые будут работать в облаке, используя облачные ресурсы для хранения данных и вычислений. Это позволяет в некоторых аспектах упростить разработку и развёртывание приложений, а так же значительно снизить затраты на поддержку и гарантии доступности.

Serverless является дочерним подходом к Cloud Native разработке. Именно этот подход и был выбран для разработки серверной части приложения PolyMap.

=== Выбор облачного провайдера с применением СППР
Наиболее крупным игроком в сфере Serverless является Amazon Web Services (AWS), однако официально в России он не доступен, что создаёт дополнительные инфраструктурные риски. По этому, в качестве облачного провайдера необходимо было выбрать отечественную альтернативу.
На текущий момент в России существует несколько крупных облачных провайдеров предоставляющих Serverless решения:
- Яндекс Облако -- крупнейший в России облачный провайдер, который активно развивает как Serverless, так и Cloud Native решения. Яндекс Облако предоставляет широкий спектр облачных услуг, включая Serverless Container, API Gateway, YDB и другие.
- Selectel -- старый игрок на российском рынке, в первую очередь ориентируется на выделенные серверы, однако Serverless решения тоже присутствуют.
- Сбер Облако (переименовали в CloudRu) -- относительно новый провайдер, активно развивается, имеет очень выгодные тарифы и бесплатные квоты.
- VK Cloud -- является ещё одним новым провайдером, Serverless решения не являются приоритетом.

Стоит отметить, что все эти хостинги соответствуют требованиям российского законодательства о хранение данных (ФЗ-152), что является важным фактором при выборе облачного провайдера.

Для выбора облачного провайдера было принято решение воспользоваться системой поддержки принятия решений (СППР), которая позволяет сравнить облачные провайдеры по заданным критериям. В качестве критериев были выбраны следующие:
- Вычисления -- функции (FaaS), наличие serverless контейнеров, технические лимиты, квоты, удобство использования
- Интеграции -- удобство вызовов функций/контейнеров (API Gateway), триггеры, очереди, CRON задачи
- БД, Очереди, Уведомления -- управления данными -- наличие serverless решений, удобство работы, поддерживаемые протоколы
- DevOps & DX -- наличие DevOps инструментов, документация, удобство автоматизации процессов. Оценивается CLI, SDK, наличие Terraform провайдера, наличие собственных GitHub/GitLab интеграций, активное сообщество (популярность)
- Мониторинг, логи -- наличие инструментов мониторинга и логирования, удобство работы с ними, внутренний функционал (создание своих графиков, алертов, язык запросов и т.д.), а так же интеграция с внешними системами.

Кроме отечественных облачных провайдеров, в таблице представлены и зарубежные:
- AWS -- на текущий момент является лидером в области Serverless, однако официально недоступен в России.
- Google Cloud Platform -- второй по популярности облачный провайдер, который активно развивает Serverless решения.
- Azure -- облачный провайдер от Microsoft, в нём тоже присутствует Serverless, однако на нём не делают акцент.

Для всех критериев были проставлены оценки по шкале от 0 до 10:
\ 0–3 -- функционал полностью отсутствует, либо пользоваться крайне неудобно.
\ 4–5 -- функционал присутствует, но выполняет базовый минимум задач, серьёзно уступает альтернативам.
\ 6–7 -- работает, но есть заметные пробелы, требует ручной настройки.
\ 8–9 -- работает хорошо, покрывает все задачи, работать удобно, но есть небольшие недочёты.
\ 10 -- самый полный и удобный на рынке функционал в данной категории.

С использованием 5 алгоритмов был рассчитан общий балл для каждого облачного провайдера. В @sppr-cloud-report находится подробный отчёт СППР. Использовалась собственная реализация СППР с открытым исходным кодом.

#figure(caption: [Результат работы СППР])[
  #show table.cell.where(y: 0): set text(weight: "bold")
  #table(
    columns: (auto, auto, auto, auto, auto, auto, auto, auto),
    align: horizon,
    table.header([Вариант], [Дом], [Блок], [Тур], [Sjp], [Sjm], [ИТОГО], [Место]),
    [*Yandex*], [5], [6], [5], [5], [6], [27], [3],
    [*Selectel*], [5], [6], [2], [2], [6], [21], [6],
    [*VK*], [5], [6], [1], [1], [6], [19], [7],
    [*Sber*], [5], [6], [3], [3], [6], [23], [5],
    [*AWS*], [7], [7], [7], [7], [7], [35], [1],
    [*Google*], [6], [6], [6], [6], [6], [30], [2],
    [*Azure*], [5], [6], [4], [4], [6], [25], [4],
  )
]<sppr-cloud-provider-result>

Как видно из таблицы #ref(<sppr-cloud-provider-result>, supplement: none), наибольшим требуемым функционалом обладает AWS, однако он, как и Google и Azure недоступны в России, наивысший балл среди отечественных сервисов получило Яндекс Облако, поэтому оно было выбрано в качестве облачного провайдера для проекта. Sber Cloud активно развивает свои Serverless решения, однако на момент написания работы, всё ещё сильно отставал от Яндекс Облака. Selectel и VK имеют очень урезанный Serverless функционал, который не позволяет реализовать проект в полном объёме.

=== Обзор Serverless компонентов Яндекс Облака
Подход к Serverless в Яндекс Облаке реализован с помощью следующих компонентов:
- Cloud Functions -- позволяет запускать код в ответ на события, такие как HTTP запросы или CRON задачи.
- Serverless Container -- позволяет запускать Docker контейнеры в ответ различные события.
- API Gateway -- позволяет описывать REST API для микросервисов в формате OpenAPI. Является публичной точкой входа в микросервисы.
- Message Queue -- позволяет организовать асинхронную связь между микросервисами. Совместим с Amazon SQS API.
- Yandex Data Base (YDB) -- Serverless база данных, которая может работать и в реляционном и в документо-ориентированном режимах. Совместимо DynamoDB API.
- Object Storage -- объектное хранилище, которое позволяет хранить и раздавать бинарные объекты по протоколу S3.
- Cloud Postbox -- сервис позволяет организовывать Email рассылки. Совместим с Amazon SES API.

В проекте не используется Cloud Functions, так как все микросервисы реализованы в виде Docker контейнеров, это позволяет использовать более привычный подход к разработке и тестированию, а так уменьшает привязанность к облачному провайдеру. Архитектура состоящая полностью из контейнеров может быть легко перенесена как на локальную машину, так и в другие облака. Кроме того, использование контейнеров позволяет использовать больший набор технологий, в том числе и те, которые не поддерживаются в Cloud Functions.

Кроме непосредственное Serverless компонентов, Яндекс Облако предоставляет и другие сервисы, которые будут полезны в проекте:
- Container Registry -- позволяет хранить Docker образы, которые будут использоваться в Serverless Container.
- Monitoring -- позволяет хранить и отслеживать состояния работы проекта: число запросов, время ответов и другие метрики. Кроме того, позволяет настраивать Alerting, который будет уведомлять о проблемах в работе проекта при срабатывании настроенных условий.
- Cloud Logging -- позволяет собирать, хранить, фильтровать и просматривать логи из всех микросервисов в одном месте.

Yandex Cloud активно развивает Serverless раздел облака, и регулярно добавляет новые компоненты и улучшения. Например, на момент написания магистерской работы, в облако был добавлен Notification Service, который позволяет отправлять персональные уведомления пользователям через SMS, Push и Email. Это позволит в будущем расширить функционал сервиса PolyMap, сильно упростив реализацию личного кабинета заказчика.

=== Content Delivery Network (CDN)
Для решения задачи географического масштабирования, необходимо использовать Content Delivery Network (CDN) сети. Они пропускают запросы пользователей через географияески ближайший к ним узел, и в случае, если запрашиваемый ресурс разрешен для кеширования и уже есть в кеше, отдают его пользователю не перенаправляя запрос на основной сервер. Главным источником трафика в сервисе является раздача карт -- это большие и тяжёлые файлы, которые будет скачивать каждый пользователь при каждом открытии карты. Сами карты являются статическими файлами, которые редко меняются, а так же, в большинстве случаев привязаны к конкретной локации. Поэтому их можно эффективно кешировать в CDN сети. Это позволит значительно снизить объём трафика и скорость открытия сайта, что положительно скажется на пользовательском опыте.

#fig("Схема работы CDN", "../assets/sheme-cdn.svg", width: 100%)<cdn>

Общий принцип работы CDN состоит в передаче DNS управления к CDN провайдеру, после чего, провайдер будет разрешать запросы к ближайшему узлу.

=== Жизненный цикл запроса
При использовании Serverless в Yandex Cloud, жизненный цикл обработки запросы выглядит следующим образом:
+ Пользователь через DNS сеть CDN определяет ближайший к нему узел, и отправляет запрос на него.
+ Узел CDN проверяет, есть ли запрашиваемый ресурс в кеше. Если он есть, то отдает его пользователю, если нет, то проксирует запрос в Yandex Cloud.
+ Yandex Cloud получает запрос и по домену находит соответствующий `API Gateway`, который будет обрабатывать запрос.
+ `API Gateway` проводит часть проверок (авторизацию, валидацию параметров) и перенаправляет запрос на соответствующий `Serverless Container`, который будет обрабатывать запрос.
+ Если нет запущенного экземпляра `Serverless Container`, то Yandex Cloud создаёт новый экземпляр контейнера из локального `Docker Registry`, и перенаправляет запрос на него.
+ `Serverless Container` обрабатывает запрос, при этом имеет возможность обращаться к прочим ресурсам Облака, таким как базы данных, очереди, и т.д.
+ После обработки запроса, `Serverless Container` возвращает ответ обратно в `API Gateway`, который в свою очередь возвращается к пользователю через CDN Proxy.
+ Если указаны заголовки кеширования, то CDN Proxy кеширует ответ на региональном узле, и в дальнейшем будет отдавать его пользователям, не перенаправляя запрос на основной сервер.

В случае обновления статического контента, есть возможность вызвать инвалидацию CDN кеша с помощью специального API. Это позволит удалить из кеша старую карту, чтобы новая версия была доступна пользователям сразу после обновления.

#fig("Инфраструктурный путь запроса", "../assets/request-lifecycle.pdf", width: 100%)<request-lifecycle>

=== Ценообразование
Основным преимуществом Serverless подхода является снижение инфраструктурных затрат относительно классических походов. Рассмотрим ценообразование в Yandex Cloud.

Важным фактором затрат на Serverless в Yandex Cloud является квоты на число бесплатно предоставляемых ресурсов. Эти квоты сбрасываются в начале каждого месяца, таким образом, низкая нагрузка в течение месяца, позволяет пользоваться инфраструктурой полностью бесплатно. После исчерпания квот, тарификация происходит по модели pay-as-you-go, то есть необходимо оплачивать только те ресурсы, которые были фактически использованы.

#include "../assets/serverless-price.typ"

=== Сравнение затрат на Serverless и Kubernetes

С использованием цен актуальных на момент написания работы (@serverless-price) можно ориентировочно рассчитать общие затраты на инфраструктуру в месяц. Возьмём ориентировочную нагрузку на пилотную версию PolyMap iOS (@polymap-ios-usage).

#include "../assets/polymap-ios-usage.typ"

Стоит отметить, что приложение не имеет официальный статус, и его не использовали для приглашений на мероприятия. В связи с этим, нагрузка на открытия приглашений сильно занижена, при реальном использовании на конференциях, число открытий приглашений будет пропорционально пришедшим пользователям.

Перенесём статистику на примерный механизм работы Serverless:
- API Gateway: 600 000 открытий карт + 70 000 просмотров аннотаций + 10 000 построений маршрутов + 250 открытий приглашений = 680 250 запросов
- Serverless Containers: 680 250 вызовов (среднее потребление: 0.5 ГБ памяти, 0.2 с процессора на вызов)
- Message Queue: 680 250 запросов
- Object Storage GET: 680 250 операций
- Исходящий трафик: ~600 000 загрузок карт по ~2 МБ = 1 200 ГБ

#include "../assets/serverless-polymap-cost.typ"

Как видно из расчёта (@polymap-serverless-cost) на самый нагруженный месяц -- сентябрь, приблизительные затраты на Serverless инфраструктуру составят *1600 рублей*, и большая часть затрат это исходящий трафик, который порождается раздачей статичных файлов карты, однако, большинство этих запросов не будут доходить для серверов, а будут обрабатываться на узлах кеша CDN, по этому реальные затраты будут значительно ниже.

Сравним это с затратами на Kubernetes кластер. Так как нагрузка не очень высокая, возьмём минимальную production конфигурацию кластера, которая будет состоять из 3 нод с 4 vCPU и 4 ГБ RAM. Три ноды позволят обеспечить доступность и отказоустойчивость сервера, а 4 vCPU и 4 ГБ RAM будет достаточно для обработки аналогичной нагрузки. Такой кластер в Yandex Cloud будет стоить *33000 рублей* в месяц, что значительно выше, чем затраты на Serverless. Для сравнения, на @selectel есть стоимость минимально возможного кластера в Selectel, он имеет меньшую отказоустойчивость, так как находится в одной зоне доступности, однако обладает большими характеристиками: 8 vCPU и 16 ГБ RAM, такой кластер будет стоить *31000 рублей* в месяц.

#subfig(
  figure(image("../assets/kubernetes-price-yandex.png"), caption: [Yandex Cloud]),
  figure(image("../assets/kubernetes-price-selectel.png"), caption: [Selectel]),
  <selectel>,
  columns: (1fr, 0.65fr),
  caption: [Стоимость Kubernetes кластера в Yandex Cloud и Selectel],
)

Так же, не стоит забывать, что кроме production кластера, необходимо будет поддерживать тестовый кластер, для разработки и тестирования новых обновлений, что значительно увеличит затраты на инфрастуктуру, в то время, как в Serverless подходе, тестовые среды даже не будут выходить из бесплатных квот. За три года активной разработки, я ни разу не использовал больше 10% от выделенных квот, что позволяет полностью бесплатно разрабатывать и тестировать новые обновления.

== Поисковая оптимизация (SEO)
Важной частью любого веб-сервиса является поисковая оптимизация (SEO – Search Engine Optimization). Этим термином называют набор методов, которые позволяют улучшить видимость сайта в поисковых системах, таких как Google, Yandex и других. Это позволяет увеличить количество пользователей, которые будут органически находить ресурс через поиск. В качестве методов SEO можно выделить:
- Оптимизация мета-тегов -- это теги в заголовке HTML страницы, которые содержат информацию о сайте: Название, описания, ключевые слова, язык
- Создание OpenGraph мета-тегов -- это специальный подвид мета-тегов, в котором описывается как будет выглядеть предпросмотр ссылки на сайт в социальных сетях.
#todo[В идеале сюда добавить сравнительную картинку с OpenGraph тегами и без них]
- Персонализация страниц -- для SPA (Single Page Application) приложений, необходимо генерировать уникальные index.html страницы в зависимости от контента, это можно сделать с помощью серверного рендеринга (SSR – Server Side Rendering) или статической генерации страниц (SSG).
- robots.txt -- специальный файл, который позволяет указать поисковым системам, какие страницы сайта не нужно индексировать.
- Sitemap.xml -- специальный файл, в котором описывается структура сайта, по которой поисковые системы могут понять, какие страницы сайта нужно индексировать, а какие нет.

=== Применение SEO в PolyMap
В сервисе PolyMap есть два основных направления SEO:
+ Лендинг страница с описанием возможностей сервиса, его преимуществами и примерами использования.
+ Веб-версия карт, которая отображает карты заказчиков.

В случае с лендинг страницей, особых сложностей в SEO нет, так как это статичный веб-сайт, который будет использовать SSG подход, что позволит поисковым движкам с лёгкостью его анализировать.

Веб-версия карт куда сложнее, это SPA приложение, которое в зависимости от URL будет подгружать карту уже после открытия сайта. При этом сами карты являются динамичным контентом, они могут добавляться новые, а старые исчезать.
SPA приложения работают следующим образом: с сервера загружается пустой index.html файл, в котором объявлен основной JavaScript файл, после скачивания которого, приложение запустит фреймворк и динамически нарисует страницу.

Для поисковой оптимизации SPA c динамическим контентом, принято используют SSR подход, при нём, на каждый запрос пользователя, сервер запускает JavaScript код приложения, и создаёт полную HTML страницу, после чего, прикрепляет состояния фрейморка в виде JSON объекта и отвечает пользователю. На стороне пользователя запускается процесс гидратации (hydration) -- это процесс, при котором фреймворк берет HTML страницу и JSON объект, и запускает приложение с уже готовым состоянием. Однако этот подход значительно увеличивает нагрузку на сервер, скорость ответа, а так же время до первого взаимодействия (Time to First Interaction -- TTFI), это происходит из-за медленного процесса гидрации.

Для PolyMap был выбран другой подход. Благодаря тому, что карты не являются текстовым контентом, можно использовать SSG только для мета-тегов, а всё остальное генерировать на стороне клиента классическим SPA подходом.
Для генерации мета-тегов достаточно подменить их в index.html файле, получаемом от фреймворка, никакие другие скрипты в процессе участвовать не будут. В проекте для этого используется отдельный микросервис `web-map-back`, который в зависимости от URL будет генерировать нужную index.html страницу, а так же, по запросу поисковых движков, будет генерировать актуальные sitemap.xml и robots.txt файлы. Подробнее о его работе будет рассказано в разделе #ref(<web-map-back>, supplement: none).

== Детальная архитектура

Детальная архитектурная схема сервиса PolyMap представлена на рисунке #ref(<detailed-architecture>, supplement: none). Её можно разделить на несколько основных типов компонентов:
- Клиенты -- это приложения с которыми взаимодействует пользователь, заказчик и художник. Они могут быть как веб-приложениями, так и нативными программами. На схеме они представлены блоками фиолетового цвета.
- Внешние сервисы -- не являются частью PolyMap, однако взаимодействуют с сервисом. Например внешние сервисы аналитики или запросы OpenGraph Meta. На схеме они представлены блоками тёмно серого цвета.
- Микросервисы -- это основные вычислительные компоненты. На схеме они представлены блоками белого цвета.
- CDN -- это прослойка между клиентами и микросервисами, которая позволяет кешировать статические файлы. На схеме она представлена блоками оранжевого цвета.
- Системы хранения -- это места хранения данных, на схеме представлены:
  - Синим цветом, префикс *`SQL`* -- SQL базы данных, например PostgreSQL или YDB в SQL режиме.
  - Зелёным цветом, префикс *`DOC`* -- документо-ориентированные базы данных, например YDB в NoSQL режиме или MongoDB.
  - Оранжевым цветом, префикс *`S3`* -- объектные хранилища S3.
  - Синим цветом, префикс *`OLAP`* -- Аналитические базы данных, например ClickHouse.

#fig(
  "Детальная схема архитектуры PolyMap",
  "../assets/polymap-detailed-architecture.pdf",
  width: 100%,
)<detailed-architecture>

Далее будут компоненты архитектуры будут рассмотрены подробнее, будет определено их назначение и обоснована значимость для сервиса.
=== Клиенты
==== Dashboard
Архитектурный компонент запланированный на будущие. Это веб-приложение с панелью управления, доступ к которому будет выдаваться каждом заказчику. В нём будет доступен список карт, которые были созданы этим заказчиком. Будет возможность оплаты и подробная информация о каждой карте.

Для каждой карты можно будет просматривать аналитику (подробнее @analytics), настраивать доступ к карте, управлять художниками, переходить к конструктору карты. Настраивать внешние интеграции (такие как расписание), просматривать список приглашений и их статистику.

Данный компонент не является критическим для работы сервиса, и его реализация не является приоритетной задачей. Он будет реализован в будущем, когда сервис будет готов к коммерческому использованию.

==== Constructor
Это компьютерное приложение, которое позволяет создавать карты. В будущем оно будет перенесено в веб-приложение с системой разграничения доступа. В текущий момент используется старая версия конструктора, разработанная в рамках бакалаврской ВКР. Оно написано на базе движка Unity3D с добавлением кастомных окон.

==== iOS App, Android App, Web Map
Клиентские приложения непосредственно отображающие карты. В рамках текущей работы, реализуется только веб-версия приложения, однако планирование архитектуры требует учёта всех версий, которые будут реализованы в будущем.

==== Landing
Это веб-сайт на котором будет размещена общая информация о сервисе, его преимуществах, возможностях и ценах. На сайте можно будет оставить заявку на создание карты. Сайт реализован в виде статического сайта, который будет раздаваться через CDN. Серверной части не требует, с другими микросервисами не взаимодействует.

=== Микросервисы
==== Dashboard-back
Этот микросервис будет реализован в будущем вместе с панелью управления. Он будет отвечать за взаимодействие с панелью управления, регистраций заказчиков, хранением сервисных настроек карт, управлением картами, расчётам аналитических графиков.

Так же, через него можно будет задать OpenGraph метаданные для карт. Он будет взаимодействовать с другими микросервисами: `map-storage`, `analytics`.

==== Constructor-WS
Это микросервис, который будет отвечать за синхронизацию совместной работы в конструкторе карты. Он будет использовать pub/sub WebSocket, клиенты конструктора будут отправлять delta-синхронизации.
На схеме архитектуры (@detailed-architecture) этот компонент помечен тегом `VPS`, что означат, что он не является частью Serverless. Это связано с тем, что на момент написания работы, в Yandex Cloud тарификация WebSocket соединений оплачивается за каждое сообщение, по цене эквивалентной обычным запросам, что делает является невыгодным. При этом, данный сервер синхронизации не требует большой вычислительной мощности и гибкого масштабирования, так как будет доступен только художникам карты только в момент редактирования карты. Поломка механизма синхронизации не приведёт к критичным последствиям. По этому он будет реализован в виде обычного виртуального сервера, на котором будет Nginx и Docker контейнер с сервисом синхронизации.

==== Blob-storage
Микросервис хранения бинарных объектов. Он может использоваться разными сервисами, но в первую очередь предназначен для конструктора карт, чтобы художники могли загружать свои собственные изображения аннотаций, планы этажей и т.д. Непосредственно хранение будет осуществляться в объектном хранилище S3, сервис будет отвечать за управления доступом. Некоторые ассеты конструктора должны быть приватными и доступны только внутри конструктора и только для определённой карты (например инженерные планировки), а некоторые должны быть публичными, такие как изображения аннотаций.

Загружать новые ассеты в хранилище смогут только художники, уровень доступа ассетов будет определяться картой из которой они были загружены.

==== Constructor-back
Микросервис обслуживающий конструктор карт. Он будет отвечать за сохранение карт из конструктора, а так же за дополнительную обработку карты и её экспорт в `map-storage`. Будет предоставлять все ассеты карты в редактируемом виде (для конструктора) с учётом авторизации художника.

==== Map-storage
Самый важный микросервис для проекта, отвечает за хранение и раздачу карт. Кроме самих файлов карт для клиентов, он будет позволять получить информацию об аннотации не скачивая всю карту целиком. Будет хранить метаданные карты, и OpenGraph метаданные в том числе картинки. Этот же микросервис будет отвечать за инвалидацию кеша CDN при обновлениях карт.

Получать информацию об аннотациях нужно будет из микросервиса `web-map-back` для генерации OpenGraph метаданных для SEO оптимизации страниц аннотаций. Например при открытие страницы `/spbstu/annotation/uuid` будет получать информацию об аннотации с `uuid` для карты `spbstu`, и генерировать OpenGraph метаданные для страницы, для конкретной аннотации. Это позволит поисковым движкам лучше индексировать страницы популярных аннотаций, и выдавать их в результатах поиска.

==== Share-back
Вспомогательный микросервис для клиентов, который отвечает за создание приглашений на карту. Приглашения состоят из аннотации на карте или маршрута и названия с описанием. Необходимо сохранить эту информацию в базе данных и вернуть пользователю уникальный короткий URL. По реализации этот сервис очень схож с классической задачей сокращение ссылок. Ничего специфичного в нём нет.

==== Qr-generator
Один из самых дискуссионных микросервисов. Он отвечает за генерацию QR-кодов, которые нужны клиентам для создания приглашений. Эти QR-коды должны быть стилизованными, должны поддаваться настройке по цветам и виду, а так же предоставляться в виде растровой `png` картинки или векторного `svg` изображения.

Может показаться, что его реализация в виде микросервиса нецелесообразна, и куда эффективнее было бы использовать клиентскую библиотеку, которая бы генерировала QR-коды прямо на клиенте в принципе без необходимости использования сервера. Однако, в дальнейшем, сервис планируется развивать до нативных мобильных приложений на других языках программирования, а стилизация QR-кодов везде должна быть одинаковой. Кроссплатформенных библиотек для стилизации QR-кодов соответствующей требованиям не существует, и пришлось бы разрабатывать её самому. Кроме того, в случае с мобильными приложениями, обновление это библиотеки потребовало бы обновления приложения, а далеко не все пользователи обновляют их своевременно. Ещё одним аргументов в пользу микросервиса является то, что генерация QR кода может быть востребована другим сервером, например для Email рассылок, в случае с которыми, пришлось бы не просто использовать библиотеку на сервере, а ещё и сохранять изображения QR-кодов в S3.

В случае с использованием микросервиса, все сложности по стилизации QR-кодов нивелируются. Сервис предоставляет REST API, в котором все параметры QR-кода определяются в виде `query` параметров, а в результате возвращается изображение. Этот подход позволяет:
- Использовать один и тот же сервис для всех платформ
- Для вложений использовать просто URL ссылку на QR-код, который будет генерироваться в момент запроса (Что полезно для Email рассылок)
- Не зависеть от обновлений библиотек, так как сервис будет поддерживаться отдельно от клиентских приложений
- Не сохранять QR-коды в S3, а генерировать их в момент запроса, что позволяет избежать лишних затрат на хранение.
- Кешировать QR-коды на CDN, что позволит избежать лишних запросов (например для рассылок)

#todo[В идеале сюда добавить пример URL для генерации QR-кода и сам QR-код]

==== Analytics<analytics>
Сервис для сбора аналитики от клиентов. Будет собирать информацию о событиях на картах и анонимизированную информацию о пользователях. Это нужно для того, что бы заказчики карт могли видеть, как пользователи взаимодействуют с картами, какие аннотации открываются чаще всего, какие маршруты строятся, как часто пользователи открывают карту, сколько их, оценивать удержание. Сервис будет сохранять информацию в ClickHouse -- это колоночная СУБД предназначенная специально для аналитики. Собственное хранилище аналитики позволяет избежать использования сторонних сервисов, таких как Google Analytics, которые могут собирать информацию о пользователях и передавать её третьим лицам. Это важно для соблюдения GDPR, ФЗ-152 и других законов о защите персональных данных. Кроме того, собственное хранение позволит бесплатно и в любых количествах фильтровать аналитику по карте и выдавать эту информацию заказчикам. Сторонние сервисы ограничивают запросы к API, особенно с дополнительными фильтрами, так как их базы данных не предназначены для этого.

==== Web-map-back<web-map-back>
Этот микросервис крайне важен для SEO оптимизации, он занимается генерацией index.html страницы для web-версии карт. В зависимости от URL запроса, он будет добавлять в сгенерированный UI фреймворком index.html файл, мета-теги, которые будут использоваться поисковыми движками для индексации страницы, а так же обслуживать нужны поисковых движков, возвращая корректные
- `sitemap.xml` -- структура сайта, в ней будут URL всех карт, которые отмечены как публичные. Эти карты будут индексироваться поисковыми движками. Например и на запрос "Карта Политеха" в поисковой системе, будет отображаться сайт PolyMap с соответствующей картой. В будущем, можно будет попробовать добавить ещё и все аннотации на каждой карте, чтоб прямо в поисковике можно было искать "Политех корпус 1, аудитория 101", и получать ссылку на карту с соответствующей аннотацией.
- `robots.txt` -- служебный файл, который позволяет указать поисковым системам, какие страницы сайта не нужно индексировать, будет использоваться для приватных карт, которые не должны индексироваться поисковыми системами.
- `.well-known/apple-app-site-association` -- специальный файл, который позволяет iOS устройствам открывать ссылки на карты в нативном приложении, а не в браузере.

==== Og-generator
Микросервис по функционалу аналогичный `qr-generator`, но генерирует не стилизацию qr кодов, а OpenGraph изображения, которые будут использоваться социальными сетями для предпросмотра ссылок на карты. Микросервис `web-map-back` будет создавать мета-теги, в том числе по протоколу OpenGraph, помещая в og:image специальные URL на микросервис `og-generator`, который по GET запросу на этот URL будет генерировать изображение. Изображение кешируется в CDN. Все параметры изображения определяются в `query` параметрах ссылки. Будет поддерживаться генерация названия карты, аннотации, возможно, предпросмотра маршрута. Кроме того, заказчики смогут кастомизировать свои OpenGraph изображения, например добавлять логотипы, в этом случае, `web-map-back` будет добавлять ссылку на эти изображения в `S3:OGImg`. Это позволит сделать `og-generator` полностью изолированным от других микросервисов, что считается хорошей архитектурной практикой.
#todo[В идеале сюда добавить картинку OpenGraph]

== Вывод
В данной главе были проанализированы требования к сервисы, определен Serverless подход к архитектуре, с помощью системы поддержки принятия решений, выбран оптимальный облачный провайдер Yandex Cloud. Так же, была подробно рассмотрена детальная архитектура, разбивка на клиенты и микросервисы, обоснована их целесообразность и взаимодействия. Рассмотрена оптимизация поисковых запросов.

В результате получилась хорошая чистая архитектура, в которой все компоненты слабо связаны между собой.

= Реализация
В этой главе будут рассмотрены некоторые детали реализации сервиса PolyMap, такие как система контроля версий, непрерывная интеграция и развёртывание (CI/CD), а так же некоторые детали реализации некоторых микросервисов. Будут выбраны языки программирования, фреймворки и основные библиотеки, которые будут составлять технический стек проекта. Так же, будет рассмотрена система мониторинга и логирования, которая будет использоваться в проекте.

== Система контроля версий
При разработке проекта в микросервисном подходе выделают два способа организации системы контроля версий:
+ Монорепозиторий -- все микросервисы хранятся в одном репозитории. Плюсом этого подходя является сквозное версионирование всех микросервисов, каждый коммит порождает новую общую версию приложения, которая гарантирована не нарушит совместимость. Минусом такого подхода является сильная связанность между микросервисами, что усложняет их независимую разработку, тестирование и развёртывание.
+ Полирепозиторий -- каждый микросервис хранится в своём репозитории. Плюсом такого подхода является независимость разработки, тестирования и развёртывания каждого микросервиса. Минусом такого подхода является сложность в управлении версиями. Может случиться так, что при обновление одного микросервиса сломается совместимость с другим.

Был выбран второй подход, так как он лучше позволяет разделить кодовые базы и вести независимую разработку. Сложность версионирования решается гарантиями обратной совместимости -- ни одна новая версия не должна ломать совместимость с предыдущими версиями. Это требование и так необходимо соблюдать, что бы корректно работало горизонтальное масштабирование, при котором в одним момент времени могут работать несколько версий одного микросервиса.

=== Выбор системы контроля версий

В качестве хранилища системы контроля версий Git можно использовать несколько систем:
- GitHub
- GitLab
- Bitbucket

Каждая из них обладает своими плюсами и минусам. Для выбора была составлена сравнительная таблица функционала, который потребуется для разработки проекта.

#include "../assets/git-services-compare.typ"

Как видно из таблицы #ref(<git-provider-comparison>, supplement: none), GitHub и GitLub имеют весь необходимый функционал и не отстают друг от друга, Bitbucket сильно отстаёт от конкурентов. GitLab имеет важное преимущество -- возможность размещать сервис полностью на своей инфрастуктуре (self-hosted), что полезно для крупных компаний с корпоративными секретам, однако не имеет особого смысла в небольших проектах. В свою очередь, облачная версия GitHub является более удобной, сильно популярнее любых конкурентов, CI/CD процессы обладают огромным количеством готовых плагинов в GitHub Marketplace. Ещё одним важным направлением развития GitHub является интеграция нейросетей на всех этапах, GitHub Copilot добавляет подсказки кода в IDE, позволяет автоматически проверять и комментировать PullRequest'ы, и даже в beta режиме полностью самостоятельно писать код по поставленным задачам.

В GitHub была создана отдельная организация, в которой расположены все репозитории связанные с проектом.
#fig("Организация Polymap в GitHub", "../assets/github-repo.jpg", width: 80%)<github-organization>

== Клиентская часть <web-map-implementation>
=== Vue
=== Three.js и WebGL
=== AppleMapKit
=== OpenGraphMeta

== Серверная часть
=== Стек технологий
#todo[Указать что для Serverless круто использовать Bun]
=== Выбор оптимальной технологии для Serverless <serverless-technology-stack>
=== Микросервисы
Рассмотрим некоторые микросервисы подробнее
=== Раздача карта и CDN



== Мониторинг
== Вывод

= Тестирование и оценка качества кода

== Непрерывная интеграция и развёртывание (CI/CD)
При разработке по микросервисной архитектуре крайне важно на самых ранних этапах автоматизировать процесс сборки и развёртывания приложения. Это обусловлено тем, что при таком подходе появляется множество проектов с частыми обновлениями, автоматизация рутинных процессов позволяет снизить количество ошибок, которые могут возникнуть при ручном развёртывании. А так же позволяет добавить автоматизированные тесты для контроля качества кода с самых ранних этапов.

=== GitHub Actions
В выбранной системе контроля версий GitHub, для автоматизации процессов сборки и развёртывания, используется встроенный инструмент GitHub Actions. С его помощью можно настроить различные автоматические процессы, которые будут запускаться при определённых действиях в репозитории. Процессы могут состоять из нескольких последовательных и параллельных шагов.

Процессы запускаются на определённом GitHub Runner'е, который может быть как облачным, так и локальным. В моём случае используются облачные GitHub Runner'ы, в качестве операционной системы используется Ubuntu 22.04. GitHub Actions позволяет запускать процессы на Windows и MacOS, однако в этом нет необходимости, весь мой стек технологий прекрасно работает на Linux.

Кроме того, GitHub Actions обладают большим количеством готовых шагов, которые можно найти в GitHub Marketplace и использовать в своих процессах автоматизации.

Процессы состоят их задач (`jobs`), каждая задача состоит из шагов (`steps`). Шаги запускаются последовательно друг за другом, задачи параллельно, однако, задачам можно указать зависимости, и следующая задача будет запускаться только после завершения всех зависимостей.

=== Infrastructure as Code (IAC)
Для автоматизации развёртывания приложений в CloudNative среде можно использовать IAC (Infrastructure as Code) подход. Он позволяет декларативно описать инфраструктуру в виде кода, который можно хранить рядом с кодом приложения в системе контроля версий. Это позволяет версионировать инфраструктуру вместе с кодом приложение, что в свою очередь позволяет откатить инфраструктуру к предыдущей версии, если в ней были внесены ошибки. Так же такой подход сильно упрощает развёртывание для Serverless приложений, который состоят из множества ресурсов, которые необходимо связать между собой.

=== Terraform
Для реализации IAC подхода наиболее популярным инструментом является Terraform. Все облачные провайдеры в первую очередь добавляют поддержку именно этого инструмента, в том числе и Yandex Cloud который используется в моём случае.

В Terraform инфраструктура описывается кодом на специальном языке HCL (HashiCorp Configuration Language). Базовой сущностью в Terraform является ресурс, который описывает отдельный компонент инфраструктуры (Api GateWay, Serverless Container, Docker Registry). Внутри блока ресурса описываются его параметры. Внутри одного Terraform проекта можно ссылаться на другие ресурсы. Например, ресурс Serverless Container должен в своих параметрах ссылаться на ресурс Docker Registry, в котором хранится образ контейнера.

#listing("Пример использования Terraform")[
  ```tf
  resource "yandex_container_registry" "registry" {
    folder_id = var.folder_id
  }

  resource "docker_image" "main" {
    name = "cr.yandex/${yandex_container_registry.registry.id}/main:latest"
    build { context = abspath(local.app_path) }
    triggers = { hash = local.project_files_hash
  }

  resource "docker_registry_image" "registry" { name = docker_image.main.name }

  resource "yandex_serverless_container" "container" {
    name               = local.project_name
    folder_id          = var.folder_id
    memory             = 128
    cores              = 1
    core_fraction      = 5
    concurrency        = 16

    image {
      url = docker_registry_image.registry.name
      digest = docker_registry_image.registry.sha256_digest
      environment = merge(var.container_env, & "LOG_VARIANT" = "JSON" })
    }

    depends_on = [...]
  }

  resource "yandex_api_gateway" "gateway" {
    name        = local.project_name
    folder_id   = var.folder_id

    spec = templatefile("${path.module}/gateway.yaml.tftpl", {
      container_id = yandex_serverless_container.container.id
    })
  }
  ```
]<terraform-example>

В примере выше (@terraform-example) описано пять ресурсов, которых полностью достаточно для микросервиса в Serverless:
+ `yandex_container_registry` -- описывает реестр Docker образов в Yandex Cloud, в котором будут храниться образы контейнеров
+ `docker_image` -- описывает образ Docker контейнера, который будет скомпилирован из `local.app_path` директории при изменении хеша от файлов проекта и будет загружен в вышеописанный реестр с тегом `latest`
+ `docker_registry_image` -- отвечает за выгрузку образа в реестр (выгрузка определяется по имени образа, который зависит от `id` реестра)
+ `yandex_serverless_container` -- описывает Serverless контейнер, к которому будет привязан `latest` образ из реестра `yandex_container_registry`, задаёт ему настройки памяти, количеству ядер, и лимиту параллельных запросов (в данным случае, контейнер не сможет обрабатывать больше 16 запросов одновременно, после выхода за пределы, будут создаваться новые экземпляры). А так же, задаются переменные окружения, которые берутся из парамеров запуска Terraform, и добавляется дополнительная переменная `LOG_VARIANT`, которая отвечает за формат логов.
+ `yandex_api_gateway` -- описывает точку входа в микросервис по спецификации OpenAPI, которая находится в отдельном `gateway.yaml.tftpl` файле, он имеет формат `yaml` с поддержкой шаблонизации Terraform. В качестве значений шаблона, в спецификацию передаётся `container_id`.

=== Преимущества Serverless в CI/CD
Совместно с использованием Terraform, Serverless подход обладает важным преимуществом -- возможностью *бесплатно* создавать сколько угодно тестовых окружений, которые будут полностью идентичны производственным, но будут полностью изолированы от внешней инфраструктуры. Это позволяет запускать тестирования на самых ранних этапах разработки, и проводить тесты на инфраструктурной среде неотличимой от производственной. Это позволяет существенно снизить количество ошибок, которые возникают из-за разницы в тестовом и производственном окружении. Например, при Kubernetes развёртывании, тестовое окружение обычно запускают на одном локальном сервере, из-за чего, при тестировании не учитываются сетевые задержки между нодами, которые неизбежно возникнут при распределённой системе.

В Yandex Cloud для разделения окружение используются *директории* -- это способ изолировать ресурсы друг от друга внутри одного облака. Ресурсы находятся в разных подсетях, к ним разные права доступа, разные IAM роли, и нет возможность взаимодействовать между ними.

Однако, на момент написания работы, процесс создания и удаления директорий занимает продолжительное время (около 5 минут), что существенно замедляло CI/CD процесс. Чтоб избежать этого ожидания, был использован классический подход -- создание Worker Pool'ов -- это группы директорий, которые создаются один раз, а потом переиспользуются по надобности. Для реализации этого подхода, был разработан и опубликован в GitHub Marketplace специальный GitHub Action `yandex-cloud-worker-folder-manager`.

#fig(
  "Диаграмма последовательности для запроса рабочих директорий",
  "../assets/yandex-cloud-worker-folder-time-diagram.pdf",
  width: 80%,
)<action-directory-worker-diagram>

Рассмотрим механизм его работы в ситуации, когда два микросервиса одновременно запрашивают новое тестовое окружение и попадают в ситуацию гонки:
+ В начале CI/CD процесса, запускается Action, который через API запрашивает в Yandex Cloud список свободных директорий (директории созданные в рамках пула определяются по наличию метки `worker-directory`, а свободные по отсутствию `worker-used-by-*`)
  + Если свободных директорий нет, то Action создаёт новую директорию, это занимает около 5 минут, после чего, помечает её меткой `worker-directory`
+ Выбирается случайная свободная директория и ей устанавливается метка `worker-used-by-<UUID>`, где `<UUID>` -- это уникальный идентификатор сгенерированный в начале CI/CD процесса на основе репозитория, коммита и места откуда он был запущен.
+ Идёт процесс ожидания в 10 секунд (этого времени точно достаточно для установки тега), после чего, запрашивается директория с нужной меткой
  + Если такой директории нет, значит другой процесс одновременно с нашим установил свою метку (ситуация гонки), и нужно повторить процесс с самого начала
+ Внутренний ID директории сохраняется в `output` переменной Action'а, которая будет использоваться в следующих шагах CI/CD процесса
+ После завершения CI/CD процесса, Action удаляет метку `worker-used-by-*` из директории, что позволяет использовать её в других процессах, есть возможность отключить эту опцию и освобождать директорию не автоматически. Например, если директория запрашивается в момент создания PullRequest'a, то:
  - на всё время пока PR открыт, за ним будет закреплена определённая директория
  - все GitHub Actions, которые будут запускаться в рамках этого PR, будут использовать эту директорию
  - на закрытие PR, нужно сделать отдельный процесс, который будет запускать Action с параметром освобождения директории

Благодаря использованию Serverless подхода, создание таких рабочих директорий в принципе не требует никаких финансовых затрат, так как весь функционал CI/CD будет генерировать минимальную нагрузку, которая не будет превышать бесплатные квоты.

#listing("Пример запроса рабочей директории в момент открытия PR")[
  ```yaml
  on:
    pull_request:
      types: [opened]

  jobs:
    create-pr-stand:
      runs-on: ubuntu-latest
      steps:
        - name: allocate folder
          id: folder
          uses: soprachevak/yandex-cloud-worker-folder@v1
          with:
            operation: get
            cloudId: ${{ secrets.YC_CLOUD_ID }}
            serviceAccountKeyJson: ${{ secrets.YC_SERVICE_ACCOUNT_KEY_FILE }}

        - name: deploy
          run: echo "FOLDER_ID=${{ steps.folder.outputs.folderId }}"
  ```
]

#listing("Пример освобождения рабочей директории в момент закрытия PR")[
  ```yaml
  on:
    pull_request:
      types: [closed]

  jobs:
    delete-pr-stand:
      runs-on: ubuntu-latest
      steps:
        - name: free folder
          id: folder
          uses: soprachevak/yandex-cloud-worker-folder@v1
          with:
            operation: free
            cloudId: ${{ secrets.YC_CLOUD_ID }}
            serviceAccountKeyJson: ${{ secrets.YC_SERVICE_ACCOUNT_KEY_FILE }}
  ```
]

=== Итоговый CI/CD Pipeline в GutHub Actions

В результате для работы каждого микросервиса было создано несколько CI/CD процессов, которые запускаются на различных этапах:
- На открытие и обновление PullRequest'a -- запускает тесты, линтеры, проверяет код на соответствие стандартам, собирает и разворачивает временное тестовое окружение
- На закрытие PullRequest'a -- удаляет временное тестовое окружение
- На `push` в `main` ветку -- публикует новую версию в продакшен
- На `push` в `dev` ветку -- публикует новую версию в тестовое окружение

При этом процессы на `push` в `main` и `dev` ветки являются одним и тем же процессом, который запускается с разными параметрами. Это позволяет гарантировать эквивалентность работы микросервиса в продакшене и тестовом окружении.

==== Процесс на открытие PullRequest'a
Запускается на открытие, переоткрытое и обновление PullRequest'a. Самый важный и комплексный процесс.

#listing("Общая структура CI процесса для PR")[
  ```yaml
  on:
    pull_request:
      types: [opened, synchronize, reopened]

  jobs:
    clear_plan_comments: ...
    pr_plan: ...
    unit: ...
    clear_deploy_comments: ...
    get_folder: ...
    terraform: ...
    comment: ...
    e2e: ...

  ```
]

#fig("Диаграмма выполнения CI процесса для PR", "../assets/pr-ci.png", width: 100%)<pr-ci-diagram>

Как видно на диаграмме (@pr-ci-diagram), процесс состоит из двух независимых частей:
+ Процесс проверки и публикации:
  + `Lint and Unit tests` -- запускает все виды статических проверок, которые можно выполнить на исходном коде. В частности, запускаются линтер, который оценивает качество кода и проверяет его на соответствие стандартам, а так же запускаются модульные тесты
  + На этом этапе параллельно запускается две задачи:
    - `Clear comments` -- удаляет старый комментарий с отчётом из текущего PR
    - `Get folder` -- запрашивает рабочую директорию в Yandex Cloud
  + `Terraform` -- применяет `Terraform` инфраструктуру в изолированную рабочую директорию из прошлого шага.
  + На этом этапе параллельно запускается две задачи:
    - `Comment deploy link` -- добавляет в PR комментарий с сгенерированным URL на API Gateway рабочей директории (это внутренний служебный поддомен Yandex Cloud, который выдаётся всем API Gateway)
    - `E2E tests` -- запускает E2E тесты, которые запускаются на реальной инфраструктуре в Yandex Cloud. Проверяют корректность работы API, верность ответов, и время ответа. На вход получают реальный URL на API Gateway, который был сгенерирован в предыдущем шаге.

+ Комментарий в PR
  + `Clear comments` -- удаляет старый комментарий с отчётом из текущего PR
  + `Terraform` -- вычисляет `Terraform` план, который будет применён в случае принятия PullRequest'a и выводит его в комментарий к PR. Это позволяет проверяющему PR увидеть, какие изменения будут применены к инфраструктуре в случае принятия PR.

В результате выполнения процесса, получается отчёт о прохождении всех тестов (@pr-ci-report), в котором описаны проблемы в качестве кода (ESLint), ошибки в модульных тестах (JEST) и результаты E2E тестов (Cypress). А так же, в комментарии к PR добавляется план Terraform, который будет применён в случая принятия PR и ссылка на API Gateway рабочей директории, с помощью которой, можно вручную протестировать изменения (@pr-ci-comment)

#fig("Отчёт о прохождении тестов", "../assets/pr-ci-report.png", width: 100%)<pr-ci-report>
#fig("Комментарий в PR после завершения CI/CD процесса", "../assets/pr-comment.png", width: 100%)<pr-ci-comment>

==== Процесс на применение изменений
Этот процесс занимается применением уже принятых PullRequest'ов в продакшен или тестовое окружение. Он запускается на `push` в `main` или `dev` ветку. Процесс упрощён так как не требует резервирования директории. И состоит всего из трёх шагов:
+ Запуск оценки качества кода и Unit тестов. Несмотря на то, что эти процессы уже были пройдены переде принятием PullRequest'a, их повторный запуск позволяет точно гарантировать отсутствие человеческого фактора (например принять PR без учёта CI проверок). Сам процесс крайне быстрый и практически не влияет на общее время выполнения.
+ Применение Terraform плана к целевому окружению.
+ Запуск E2E тестов на целевом окружении, который позволит *гарантировать*, что уже применённые изменения не повлияли на работоспособность сервиса.

#fig(
  "Диаграмма выполнения CI процесса на применение изменений",
  "../assets/dev-ci.png",
  width: 100%,
)<dev-ci-diagram>

== Оценка качества кода
Оценка качества кода -- это не менее важный этап разработки, чем тестирование, однако, многие разработчики пренебрегают им, из-за чего, в последствие код становится сложным в поддержке, усложняется его понимание, а так же, увеличивается количество ошибок, которые можно было бы избежать на этапе статического анализа.

В целом, оценка качества кода это комплексный процесс, и не все этапы могут быть автоматизированы, однако выделяют следующие объективные метрики:
- Читаемость (readability)
  - Ясные и осмысленные имена переменных, функций, классовq
  - Чёткая структура и форматирование.
  - Отсутствие дублирования кода.
- Поддерживаемость (maintainability)
  - Разделение ответственности
  - Модульность архитектуры
  - Чёткая и единообразная структура кода/стиля
- Тестируемость (testability)
  - Наличие модульных, интеграционный, e2e и UI тестов
  - Удобство тестирования (использование DI и интерфейсов)
- Производительность (performance)
  - Использования эффективных алгоритмов и структур данных
- Надёжность (reliability)
  - Обработка исключений
  - Проверка граничных условий
  - Проверка входных данных (от пользователей)
- Безопасность (security)
  - Защита от известных уязвимостей (SQL инъекции, XSS, CSRF)
  - Разделение прав доступа

=== Применение инструментов оценки качества кода в PolyMap
В контексте данной работы, сервис PolyMap исполняет практически все требования к качеству кода. Часть требований выполняется автоматически в следствие выбранной архитектуры, а в частности:
- в следствие использования микросервисов выполняется разделение ответственности, модульность архитектуры
- в следствие использования CloudNative подхода, выполняется разделение прав доступа (каждый микросервис имеет свой сервисный аккаунт, который имеет доступ только к тем ресурсам, которые необходимы для работы микросервиса)
- использования современных фреймворков и библиотек, реализует защиту от известных уязвимостей
  - параметризация в клиенте для базы данных, защита от SQL инъекций
  - Hono использует JWT токены для аутентификации и авторизации, что позволяет избежать CSRF атак
  - использование VueJS позволяет избежать XSS атак, так как он экранирует все пользовательские данные (настроен на запрет использования `v-html` директивы, которая позволяет вставлять HTML код в шаблон)
- использование TypeScript позволяет избежать ошибок в коде, связанных с неправильным использованием типов данных, в режиме `strict` проверяются все типы данных, и предупреждает об ошибках использования типов на этапе компиляции.

Для выполнения остальных требований, требуется использовать специализированные инструменты, в случае с PolyMap, это:
- ESLint -- статический анализатор кода для JavaScript и TypeScript, позволяет проверять код на соответствие стандартам, на наличие ошибок и потенциальных проблем. В PolyMap во всех проектах используется ESLint c настройками:
  - `@typescript-eslint/recommended` -- проверять код на соответствие стандартам TypeScript, а так же, на наличие ошибок (названия переменных, стилистика кода, неиспользуемые переменные, и т.д.).
  - `eslint-plugin-import` -- позволяет проверять корректность импортов в проекте.
  - `eslint-plugin-prettier` -- позволяет проверять код на соответствие стандартам форматирования кода.
  ESLint запускается в CI/CD процессе на каждом этапе, и при наличии ошибок, процесс останавливается, и PullRequest не может быть принят. Это позволяет гарантировать отсутствие ошибок в коде. Кроме того, при правильной настройке IDE, ESLint работает в реальном времени и отображает ошибки и предупреждения прямо во время написания кода.
- Модульное тестирование -- используется во всем микросервисах, запускается в CI/CD процессе на каждом этапе, и в случае ошибок, аналогично не позволяет принять PR. В качестве инструмента используется встроенный инструмент BunJS.
- Проверка входных данных в API -- используется в каждом микросервисе как middleware для фреймворка Hono. Используется zod-validator, который позволяет проверять входные данные на соответствие заданной схеме и строго типизировать проверенные данные, что на уровне TypeScript гарантирует использование только корректных данных.

#listing("Пример использования zod-validator")[
  ```ts
  const schema = z.object({
     foo: z.string().length(5),
     bar: z.number()
  })

  api.post('/example',
    zValidator('json', schema),
    async (c) => {
      const valid = c.req.valid('json')
      console.log(valid.foo) // string with length 5
      console.log(valid.bar) // number
  })
  ```
]

== Модульное тестирование

Модульное тестирование важный этап разработки, он позволяет проверить отдельные части кода на корректность работы. В отличие от интеграционного и e2e тестирования, модульное тестирование проверяет только отдельные функции и классы, без учёта их взаимодействия с другими сервисами. Это позволяет проверить код на соответствие спецификации, однако не гарантирует его корректную работу в реальной среде. Важным преимуществом является скорость выполнения тестов, для них не требуется развёртывать инфраструктуру, и в контексте TypeScript, нет необходимости компилировать приложение. В PolyMap все микросервисы используют модульное тестирование с помощью встроенного в Bun инструмента `bun:test`. Это быстрый и мощный инструмент, который позволяет запускать TypeScript и JavaScript тесты, по синтаксису совместим с самым популярным тестовым инструментом `jest`, но работает быстрее и имеет дополнительные возможности. Bun Test поддерживает классические тесты, обладает Lifecycle hooks (`beforeAll`, `afterAll`, `beforeEach`, `afterEach`), поддерживает асинхронные тесты, Snapshot testing, и Mocking.

#listing("Пример модульного теста Bun test")[
  ```ts
  import { expect, test } from "bun:test";

  test("2 + 2", () => {
    expect(2 + 2).toBe(4);
  });
  ```
]

Для совместимости с тестами, код должен быть написан с учётом тестирования, а именно использовать Dependency Injection (DI) и интерфейсы, это позволит легко подменять зависимости в тестах используя Mock объекты вместо реальных зависимостей, благодаря чему, тесты лучше изолированы друг от друга.

#listing("Пример использования Mock в Bun test")[
  ```ts
  import { test, expect, mock } from "bun:test";
  const random = mock(() => Math.random());

  test("random", async () => {
    const val = random();
    expect(val).toBeGreaterThan(0);
    expect(random).toHaveBeenCalled();
    expect(random).toHaveBeenCalledTimes(1);
  });
  ```
]

Кроме того, Bun test поддерживает Mocking встроенных модулей и npm пакетов, путём подмены модулей в `preload` режиме, благодаря чему, обычный `import` модуля получит Mock версию, а не реальную.

#listing("Пример использования Mock для подмены модуля Bun test")[
  ```ts
  import { test, expect, mock } from "bun:test";

  mock.module("./module", () => {
    return {
      foo: "bar",
    };
  });

  test("mock.module", async () => {
    const esm = await import("./module");
    expect(esm.foo).toBe("bar");

    const cjs = require("./module");
    expect(cjs.foo).toBe("bar");
  });
  ```
]


== UI тестирование
Тестирование пользовательского интерфейса не так популярно как модульное или интеграционное, это связано с тем, что UI тесты писать достаточно сложно и выполняются они зачастую медленно, особенно, если технология UI не поддерживает такой вид тестов.

== End to End тестирование
== Нагрузочное тестирование

